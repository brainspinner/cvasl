{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notepad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be processed using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # using operating system dependent functionality (folders)\n",
    "import glob\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import numpy as np    # numerical computing (manipulating and performing operations on arrays of data)\n",
    "import copy     # Can Copy and Deepcopy files so original file is untouched.\n",
    "from ipywidgets import IntSlider, Output\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "#import SimpleITK as sitk\n",
    "import skimage\n",
    "import hashlib\n",
    "import sys\n",
    "sys.path.insert(0, '../') # path to functions\n",
    "from brainspin import file_handler as fh # \n",
    "from brainspin import mold #\n",
    "from brainspin import carve\n",
    "from brainspin.file_handler import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dicom stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brainspin.carve import PydicomDicomReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicomfile_directory1 = \"../tests/dicom_example_folder\"\n",
    "source_column = 'file'\n",
    "# reader = PydicomDicomReader(\n",
    "#         exclude_fields=('PatientName',),\n",
    "#     )\n",
    "# source = carve.DirectorySource(dicomfile_directory1, source_column)\n",
    "# df = reader.read(source)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_folder = 'test'\n",
    "# try: \n",
    "#     os.mkdir(output_folder)\n",
    "# except FileExistsError:\n",
    "#     pass \n",
    "\n",
    "# jpg = PydicomDicomReader.rip_out_jpgs(dicomfile_directory1,source,output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hash_folder(origin_folder1, file_extension, made, force=False):\n",
    "    \"\"\"Hashing function to be used by command line.\n",
    "\n",
    "    :param origin_folder1: The string of the folder with files to hash\n",
    "    :type origin_folder1: str\n",
    "    :param file_extension: File extension\n",
    "    :type file_extension: str\n",
    "    :param made: file directory where csv with hashes will be put\n",
    "    :type made: str\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(made, 'hash_output.csv')\n",
    "    df = hash_rash(origin_folder1, file_extension)\n",
    "    if not force:\n",
    "        if os.path.isfile(filepath):\n",
    "            return\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filepath))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    df.to_csv(filepath)\n",
    "\n",
    "# def save_preprocessed(array, out_fname, force):\n",
    "#     \"\"\"\n",
    "#     This function is written to be called by the cli module.\n",
    "#     It stores arrays in a directory.\n",
    "#     \"\"\"\n",
    "#     if not force:\n",
    "#         if os.path.isfile(out_fname):\n",
    "#             return\n",
    "#     try:\n",
    "#         os.makedirs(os.path.dirname(out_fname))\n",
    "    # except FileExistsError:\n",
    "    #     pass\n",
    "    # np.save(out_fname, array, allow_pickle=False)\n",
    "\n",
    "\n",
    "def hash_rash(origin_folder1, file_extension):\n",
    "    \"\"\"Hashing function to check files are not corrupted or to assure\n",
    "    files are changed.\n",
    "\n",
    "    :param origin_folder1: The string of the folder with files to hash\n",
    "    :type origin_folder1: str\n",
    "    :param file_extension: File extension\n",
    "    :type file_extension: str\n",
    "\n",
    "    :returns: Dataframe with hashes for what is in folder\n",
    "    :rtype: ~pandas.DataFrame\n",
    "    \"\"\"\n",
    "    hash_list = []\n",
    "    file_names = []\n",
    "    files = '**/*.' + file_extension\n",
    "    \n",
    "    non_suspects1 = glob.glob(\n",
    "        os.path.join(origin_folder1, files),\n",
    "        recursive=True,\n",
    "    )\n",
    "    # print(non_suspects1)\n",
    "    BUF_SIZE = 65536\n",
    "    for file in non_suspects1:\n",
    "        sha256 = hashlib.sha256()\n",
    "        with open(file, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(BUF_SIZE)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha256.update(data)\n",
    "        result = sha256.hexdigest()\n",
    "        hash_list.append(result)\n",
    "        file_names.append(file)\n",
    "        #print(file_names)\n",
    "    df = pd.DataFrame(hash_list, file_names)\n",
    "    df.columns = [\"hash\"]\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={'index': 'file_name'})\n",
    "    #df.to_csv('out.csv')\n",
    "    print(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_2DF():\n",
    "\n",
    "    date = pd.date_range('today', periods=20)\n",
    "    DF1 = pd.DataFrame(np.random.rand(20, 2), index=date)\n",
    "\n",
    "    #DF2 = pd.DataFrame(np.random.rand(20, 4), index=date, columns='A B C D'.split())\n",
    "\n",
    "    return DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF1 = return_2DF()\n",
    "DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fh.hash_folder('../not_pushed', 'gz', 'hope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_directory = '../not_pushed'\n",
    "file_directory_list = glob.glob(\n",
    "    os.path.join(file_directory, '**/*.tsv'),\n",
    "    recursive=True,\n",
    "    )\n",
    "for file in file_directory_list:\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# non_suspects1 = glob.glob(os.path.join('../not_pushed','**/*.tsv' ))\n",
    "# print(non_suspects1)\n",
    "for file in files:\n",
    "        if file.endswith('.tsv'):\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the config pathways for the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "root_mri_directory = config.get_directory('root_mri_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsv_pattern = os.path.join(root_mri_directory, '**/*.tsv')\n",
    "tsv_files = glob.glob(tsv_pattern, recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_example = pd.read_csv(tsv_files[0], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mri_pattern = os.path.join(root_mri_directory, '**/*.gz')\n",
    "gz_files = glob.glob(mri_pattern, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gz_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we can use something not in the base environment just to check that these files exist correctly\n",
    "\n",
    "\n",
    "\n",
    "# A path to an mrid brain .nii image:\n",
    "t1_fn = gz_files[0]\n",
    "\n",
    "# Read the .nii image containing the volume with SimpleITK:\n",
    "sitk_t1 = sitk.ReadImage(t1_fn)\n",
    "\n",
    "# and access the numpy array:\n",
    "t1 = sitk.GetArrayFromImage(sitk_t1)\n",
    "\n",
    "# now display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.express as px\n",
    "\n",
    "\n",
    "# fig = px.imshow(\n",
    "#     t1,\n",
    "#     facet_col=1,\n",
    "#     animation_frame=0,\n",
    "#     binary_string=True,\n",
    "#     binary_format=\"jpg\",\n",
    "# )\n",
    "# fig.layout.annotations[0][\"text\"] = \"Something0\"\n",
    "# fig.layout.annotations[1][\"text\"] = \"Something2\"\n",
    "# plotly.io.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitk.Show(sitk_t1, debugOn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "data = t1\n",
    "z, x, y = data.nonzero()\n",
    "ax.scatter(x, y, z, c=z, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
