{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ML testing: experiment #1\n",
    "\n",
    "This notebook involves testing for the MRI conference abstract. This notebook shows TOP based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os       # using operating system dependent functionality (folders)\n",
    "import sys\n",
    "import glob\n",
    "from functools import reduce\n",
    "\n",
    "# base ds libraries\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import numpy as np    # numerical computing (manipulating and performing operations on arrays of data)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ml stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "sys.path.insert(0, '../') # path to functions\n",
    "import cvasl.seperated as sep\n",
    "from cvasl.file_handler import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath_mri = '../open_work/internal_results/cleaned_pvc2s/' \n",
    "filename_mri = os.path.join(filepath_mri,'StrokeMRI_pvc2c.csv') \n",
    "filepath_top = '../open_work/internal_results/cleaned_pvc2s/' \n",
    "filename_top = os.path.join(filepath_top,'TOP_pvc2c.csv') \n",
    "filepath_sabre = '../open_work/internal_results/cleaned_pvc2s/' \n",
    "filename_sabre = os.path.join(filepath_top,'SABRE_pvc2_cleaned.csv') \n",
    "filepath_insight46 = '../open_work/internal_results/cleaned_pvc2s/' \n",
    "filename_insight46 =  os.path.join(filepath_top,'Insight46_pvc2c.csv') \n",
    "# read in data\n",
    "TOP = pd.read_csv(filename_top)\n",
    "StrokeMRI = pd.read_csv(filename_mri)\n",
    "SABRE = pd.read_csv(filename_sabre)\n",
    "Insight46 = pd.read_csv(filename_insight46)\n",
    "# take extra column off\n",
    "TOP = TOP.drop(TOP.columns[0],axis=1)\n",
    "SABRE = SABRE.drop(SABRE.columns[0],axis=1)\n",
    "StrokeMRI = StrokeMRI.drop(StrokeMRI.columns[0],axis=1)\n",
    "Insight46 = Insight46.drop(Insight46.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we need to flip the sex back to numbers for a correlation\n",
    "sex_mapping = {'F':0,'M':1}\n",
    "TOP = TOP.assign(sex = TOP.sex.map(sex_mapping))\n",
    "#TOP.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "StrokeMRI = StrokeMRI.assign(sex = StrokeMRI.sex.map(sex_mapping))\n",
    "#StrokeMRI.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Insight46 = Insight46.assign(sex = Insight46.sex.map(sex_mapping))\n",
    "#Insight46.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coly = TOP.columns\n",
    "SABRE = SABRE.assign(sex = SABRE.sex.map(sex_mapping))\n",
    "SABRE = SABRE[coly]\n",
    "SABRE = SABRE.dropna()\n",
    "#SABRE.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Build ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ml_matrix = TOP.drop('participant_id', axis=1)\n",
    "X = ml_matrix.drop('age', axis =1)\n",
    "X = X.values\n",
    "X = X.astype('float')\n",
    "y = ml_matrix['age'].values\n",
    "y=y.astype('float')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # y_split will be used for stratified sampling by sex in the splits\n",
    "# # this way each fold will have the same proportion of 'sex' labels as the whole dataset\n",
    "# y_split = ml_matrix['sex'].values\n",
    "# # 5 folds as example, you can change this\n",
    "# sss = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=16)\n",
    "# sss.get_n_splits(X, y_split)\n",
    "# print(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'Whole dataset shape: X {X.shape}, y {y.shape}')\n",
    "# unique, counts = np.unique(y_split, return_counts=True)\n",
    "# print(f'Classes: {unique}, percentages: {100*counts/y.shape[0]}')\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(sss.split(X, y_split)):\n",
    "#     print(f\"\\nFold {i}:\")\n",
    "#     print(f'Train shapes: X {X[train_index].shape}, y {y[train_index].shape}')\n",
    "#     unique, counts = np.unique(y_split[train_index], return_counts=True)\n",
    "#     print(f'Sex classes: {unique}, percentages: {100*counts/y[train_index].shape[0]}')\n",
    "#     print(f'\\nTest shapes: X {X[test_index].shape}, y {y[test_index].shape}')\n",
    "#     unique, counts = np.unique(y_split[test_index], return_counts=True)\n",
    "#     print(f'Sex classes: {unique}, percentages: {100*counts/y[test_index].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def frame_a_model_sex_split(model_name, model_file_name, scikit_model, our_ml_matrix, our_x, our_y,):\n",
    "    \"\"\"\n",
    "    This takes a sci-kit learn coded model and creates a dataframe based on k-folds of results on\n",
    "    our_ml_matrix, and it's X component\n",
    "    returns a dataframe of fold results\n",
    "    and raw y_test versus y_pred\n",
    "    \"\"\"\n",
    "    y_split = our_ml_matrix['sex'].values\n",
    "# 5 folds as example, you can change this\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=12)\n",
    "    X= our_x\n",
    "    y =our_y\n",
    "    sss.get_n_splits(X, y_split)\n",
    "    print(sss)\n",
    "    print(f'Whole dataset shape: X {X.shape}, y {y.shape}')\n",
    "    unique, counts = np.unique(y_split, return_counts=True)\n",
    "    print(f'Classes: {unique}, percentages: {100*counts/y.shape[0]}')\n",
    "    y_frame = []\n",
    "    all_mod_results = []\n",
    "    models = []\n",
    "    for i, (train_index, test_index) in enumerate(sss.split(X, y_split)):\n",
    "        print(f\"\\nFold {i}:\")\n",
    "        print(f'Train shapes: X {X[train_index].shape}, y {y[train_index].shape}')\n",
    "        unique, counts = np.unique(y_split[train_index], return_counts=True)\n",
    "        print(f'Sex classes: {unique}, percentages: {100*counts/y[train_index].shape[0]}')\n",
    "        print(f'\\nTest shapes: X {X[test_index].shape}, y {y[test_index].shape}')\n",
    "        unique, counts = np.unique(y_split[test_index], return_counts=True)\n",
    "        print(f'Sex classes: {unique}, percentages: {100*counts/y[test_index].shape[0]}')\n",
    "        cols = ['algorithm','fold', 'file_name','mae', 'r2', 'explained_variance']\n",
    "        mod_results = pd.DataFrame(columns=cols)\n",
    "        current_fold_X_train = X[train_index]\n",
    "        current_fold_y_train = y[train_index]\n",
    "        current_fold_X_test = X[test_index]\n",
    "        current_fold_y_test = y[test_index]\n",
    "        scikit_model.fit(current_fold_X_train, current_fold_y_train)\n",
    "        current_fold_y_pred = scikit_model.predict(current_fold_X_test)\n",
    "\n",
    "        data= [[\n",
    "            model_name,\n",
    "            i,\n",
    "            model_file_name,\n",
    "            mean_absolute_error(current_fold_y_test, current_fold_y_pred),\n",
    "            scikit_model.score(current_fold_X_test, current_fold_y_test),\n",
    "            metrics.explained_variance_score(current_fold_y_test, current_fold_y_pred)]]\n",
    "        mod_results_current_fold = pd.DataFrame(data, columns=cols)\n",
    "        mod_results = pd.concat([mod_results, mod_results_current_fold])\n",
    "        mod_results.reset_index(drop=True, inplace=True)\n",
    "        all_mod_results.append(mod_results)\n",
    "        #print(i)\n",
    "        #print(mod_results)\n",
    "        y_frame_now = pd.DataFrame(\n",
    "            {'y_test': list(current_fold_y_test),\n",
    "             'y_pred': list(current_fold_y_pred)})\n",
    "\n",
    "        y_frame.append(y_frame_now)\n",
    "        models.append(scikit_model)\n",
    "        #print(type(all_mod_results[0]))\n",
    "    df = pd.concat(all_mod_results) \n",
    "    y_frame = pd.concat([\n",
    "        y_frame[0],\n",
    "        y_frame[1],\n",
    "        y_frame[2],\n",
    "        y_frame[3],\n",
    "        y_frame[4],], axis=0)\n",
    "    \n",
    "    return df, y_frame, models #all_mod_results\n",
    "\n",
    "\n",
    "linr_k_frame, linr_y_frame, models = sep.frame_a_model_sex_split('linear regression', 'unharm_top_linr', LinearRegression(), ml_matrix, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linr_k_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_linr = sep.avg_k_folds(linr_k_frame)\n",
    "avg_linr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linr_y_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linr = models\n",
    "linr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## optional save models\n",
    "#joblib.dump(linr[0], ('../result_models/'+ 'unharm_top_linr0.sav'))\n",
    "#joblib.dump(linr[1], ('../result_models/'+ 'unharm_top_linr1.sav'))\n",
    "#joblib.dump(linr[2], ('../result_models/'+ 'unharm_top_linr2.sav'))\n",
    "#joblib.dump(linr[3], ('../result_models/'+ 'unharm_top_linr3.sav'))\n",
    "#joblib.dump(linr[4], ('../result_models/'+ 'unharm_top_linr4.sav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cols = ['algorithm','fold', 'file_name','mae', 'r2', 'explained_variance']\n",
    "# linr_results = pd.DataFrame(columns=cols)\n",
    "# y_frame = []\n",
    "# for i, (train_index, test_index) in enumerate(sss.split(X, y_split)):\n",
    "#     linr = LinearRegression()\n",
    "#     current_fold_X_train = X[train_index]\n",
    "#     current_fold_y_train = y[train_index]\n",
    "#     current_fold_X_test = X[test_index]\n",
    "#     current_fold_y_test = y[test_index]\n",
    "#     linr.fit(current_fold_X_train, current_fold_y_train)\n",
    "#     current_fold_y_pred = linr.predict(current_fold_X_test)\n",
    "\n",
    "#     data= [[\n",
    "#         'linear regression',\n",
    "#         i,\n",
    "#         'unharm_top_linr.sav',\n",
    "#         mean_absolute_error(current_fold_y_test, current_fold_y_pred),\n",
    "#         linr.score(current_fold_X_test, current_fold_y_test),\n",
    "#         metrics.explained_variance_score(current_fold_y_test, current_fold_y_pred)]]\n",
    "#     linr_results_current_fold = pd.DataFrame(data, columns=cols)\n",
    "#     linr_results = pd.concat([linr_results, linr_results_current_fold])\n",
    "#     linr_results.reset_index(drop=True, inplace=True)\n",
    "#     y_frame_now = pd.DataFrame(\n",
    "#         {'y_test': list(current_fold_y_test),\n",
    "#          'y_pred': list(current_fold_y_pred)})\n",
    "    \n",
    "#     y_frame.append(y_frame_now)\n",
    "    \n",
    "\n",
    "# linr_results\n",
    "# #y_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_frame_linr = pd.concat([\n",
    "#     y_frame[0],\n",
    "#     y_frame[1],\n",
    "#     y_frame[2],\n",
    "#     y_frame[3],\n",
    "#     y_frame[4],], axis=0)\n",
    "# y_frame_linr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- mae_mean = linr_results.mae.mean()\n",
    "r2_mean =  linr_results.r2.mean()\n",
    "explained_variance =  linr_results.explained_variance.mean()\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# avg_linr = sep.avg_k_folds(linr_results)\n",
    "# avg_linr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional model save\n",
    "#joblib.dump(linr, ('../result_models/'+ 'unharm_top_linr.sav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data= [[\n",
    "#     'linear regression',\n",
    "#     'unharm_top_linr.sav',\n",
    "#     linr_results.mae.mean(),\n",
    "#     linr_results.r2.mean(),\n",
    "#     linr_results.explained_variance.mean()]]\n",
    "# linr_results_average = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "# linr_results_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llreg_k_frame, lreg_y_frame, model = sep.frame_a_model_sex_split('lasso regression', 'unharm_top_llreg', linear_model.LassoLars(alpha=0.01), ml_matrix, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llreg_k_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_llreg = sep.avg_k_folds(llreg_k_frame)\n",
    "avg_llreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lreg_y_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['algorithm','fold', 'file_name','mae', 'r2', 'explained_variance']\n",
    "llreg_results = pd.DataFrame(columns=cols)\n",
    "y_frame = []\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X, y_split)):\n",
    "    llreg = linear_model.LassoLars(alpha=0.01)\n",
    "    current_fold_X_train = X[train_index]\n",
    "    current_fold_y_train = y[train_index]\n",
    "    current_fold_X_test = X[test_index]\n",
    "    current_fold_y_test = y[test_index]\n",
    "    llreg.fit(current_fold_X_train, current_fold_y_train)\n",
    "    current_fold_y_pred = llreg.predict(current_fold_X_test)\n",
    "\n",
    "    data= [[\n",
    "        'lasso linear regression',\n",
    "        i,\n",
    "        'unharm_top_llreg.sav',\n",
    "        mean_absolute_error(current_fold_y_test, current_fold_y_pred),\n",
    "        llreg.score(current_fold_X_test, current_fold_y_test),\n",
    "        metrics.explained_variance_score(current_fold_y_test, current_fold_y_pred)]]\n",
    "    llreg_results_current_fold = pd.DataFrame(data, columns=cols)\n",
    "    llreg_results = pd.concat([llreg_results, llreg_results_current_fold])\n",
    "    llreg_results.reset_index(drop=True, inplace=True)\n",
    "    y_frame_now = pd.DataFrame(\n",
    "        {'y_test': list(current_fold_y_test),\n",
    "         'y_pred': list(current_fold_y_pred)})\n",
    "    \n",
    "    y_frame.append(y_frame_now)\n",
    "    \n",
    "\n",
    "llreg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_frame_llreg = pd.concat([\n",
    "    y_frame[0],\n",
    "    y_frame[1],\n",
    "    y_frame[2],\n",
    "    y_frame[3],\n",
    "    y_frame[4],], axis=0)\n",
    "y_frame_llreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'lasso regression',\n",
    "    'unharm_top_llreg.sav',\n",
    "    llreg_results.mae.mean(),\n",
    "    llreg_results.r2.mean(),\n",
    "    llreg_results.explained_variance.mean()]]\n",
    "llreg_results_average = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "llreg_results_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = llreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'lasso linear regression',\n",
    "    'unharm_top_lassor.sav',\n",
    "    mean_absolute_error(y_test, y_pred),\n",
    "    llreg.score(X_test,y_test),\n",
    "    metrics.explained_variance_score(y_test, y_pred)]]\n",
    "llreg_results = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#llreg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llreg_y_test = y_test\n",
    "llreg_y_pred = y_pred\n",
    "llreg_compare = pd.DataFrame(\n",
    "    {'y_test_real_age': llreg_y_test,\n",
    "     'lasso_y_pred_age': llreg_y_pred,\n",
    "    })\n",
    "llreg_compare = llreg_compare.reset_index()\n",
    "#llreg_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeRegressor(random_state=7)\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_pred = dtree.predict(X_test)\n",
    "# print('R2 score dtree regression: %.3f' % dtree.score(X_test,y_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_test, y_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data= [[\n",
    "    'decision tree',\n",
    "    'unharm_top_dtree.sav',\n",
    "    mean_absolute_error(y_test, y_pred),\n",
    "    dtree.score(X_test,y_test),\n",
    "    metrics.explained_variance_score(y_test, y_pred)]]\n",
    "dtree_results = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#dtree_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtree_y_test = y_test\n",
    "dtree_y_pred = y_pred\n",
    "dtree_compare = pd.DataFrame(\n",
    "    {'y_test_real_age': dtree_y_test,\n",
    "     'dtree_y_pred_age': dtree_y_pred,\n",
    "    })\n",
    "dtree_compare = dtree_compare.reset_index()\n",
    "#dtree_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regr = MLPRegressor(random_state=1, max_iter=900)\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score neural network mlp regression: %.3f' % regr.score(X_test,y_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_test, y_pred))\n",
    "# print('MAE: % .3f' % mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'MLP regression',\n",
    "    'unharm_top_regr.sav',\n",
    "    mean_absolute_error(y_test, y_pred),\n",
    "    regr.score(X_test,y_test),\n",
    "    metrics.explained_variance_score(y_test, y_pred)]]\n",
    "regr_results = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#regr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regr_y_test = y_test\n",
    "regr_y_pred = y_pred\n",
    "regr_compare = pd.DataFrame(\n",
    "    {'y_test_real_age': regr_y_test,\n",
    "     'regr_y_pred_age': regr_y_pred,\n",
    "    })\n",
    "regr_compare = regr_compare.reset_index()\n",
    "#regr_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svr_p2 = SVR(C=1.0, kernel='poly', degree =2, epsilon=0.2)\n",
    "svr_p2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = svr_p2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score SVR 2nd degree poly kernel regression: %.3f' % svr_p2.score(X_test,y_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_test, y_pred))\n",
    "# print('MAE: % .3f' % mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'SVR poly2',\n",
    "    'unharm_top_svrp2.sav',\n",
    "    mean_absolute_error(y_test, y_pred),\n",
    "    svr_p2.score(X_test,y_test),\n",
    "    metrics.explained_variance_score(y_test, y_pred)]]\n",
    "svr_p2_results = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#svr_p2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svrp2_y_test = y_test\n",
    "svrp2_y_pred = y_pred\n",
    "svrp2_compare = pd.DataFrame(\n",
    "    {'y_test_real_age': svrp2_y_test,\n",
    "     'svrp2_y_pred_age': svrp2_y_pred,\n",
    "    })\n",
    "svrp2_compare = svrp2_compare.reset_index()\n",
    "#svrp2_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eregr = ElasticNetCV(cv=5, random_state=17)\n",
    "eregr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = eregr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_pred = eregr.predict(X_test)\n",
    "# print('R2 score elasticnetcv regression: %.3f' % eregr.score(X_test,y_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_test, y_pred))\n",
    "# print('MAE: % .3f' % mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Elastic_netCV',\n",
    "    'unharm_top_eregr.sav',\n",
    "    mean_absolute_error(y_test, y_pred),\n",
    "    eregr.score(X_test,y_test),\n",
    "    metrics.explained_variance_score(y_test, y_pred)]]\n",
    "eregr_results = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#eregr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eregr_y_test = y_test\n",
    "eregr_y_pred = y_pred\n",
    "eregr_compare = pd.DataFrame(\n",
    "    {'y_test_real_age': eregr_y_test,\n",
    "     'eregr_y_pred_age': eregr_y_pred,\n",
    "    })\n",
    "eregr_compare = eregr_compare.reset_index()\n",
    "#eregr_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "etreg = ExtraTreesRegressor(n_estimators=100, random_state=0)\n",
    "etreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = etreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_pred = etreg.predict(X_test)\n",
    "# print('R2 score extra trees regression: %.3f' % etreg.score(X_test,y_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_test, y_pred))\n",
    "# print('MAE: % .3f' % mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Extra trees',\n",
    "    'unharm_top_etreg.sav',\n",
    "    mean_absolute_error(y_test, y_pred),\n",
    "    etreg.score(X_test,y_test),\n",
    "    metrics.explained_variance_score(y_test, y_pred)]]\n",
    "etreg_results = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#etreg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "etreg_y_test = y_test\n",
    "etreg_y_pred = y_pred\n",
    "etreg_compare = pd.DataFrame(\n",
    "    {'y_test_real_age': etreg_y_test,\n",
    "     'etreg_y_pred_age': etreg_y_pred,\n",
    "    })\n",
    "etreg_compare = etreg_compare.reset_index()\n",
    "#etreg_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_based_unharmonized_on_top =pd.concat([linr_results,\n",
    "                   llreg_results,\n",
    "                   dtree_results,\n",
    "                   regr_results,\n",
    "                   svr_p2_results,\n",
    "                   eregr_results,\n",
    "                  etreg_results],\n",
    "                  axis=0)\n",
    "top_based_unharmonized_on_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_frames1 = [linr_compare, llreg_compare, dtree_compare,]# regr_compare, ]#etreg_compare, svrp2_compare,]\n",
    "real_versus_projected_y1 = reduce(lambda  left,right: pd.merge(left,right,on=[\"index\"],\n",
    "                                            how='outer'), data_frames1)\n",
    "#real_versus_projected_y1\n",
    "data_frames2 = [eregr_compare, svrp2_compare, etreg_compare,]\n",
    "real_versus_projected_y2 = reduce(lambda  left,right: pd.merge(left,right,on=[\"index\"],\n",
    "                                            how='outer'), data_frames2)\n",
    "#real_versus_projected_y2\n",
    "real_versus_projected_y2 = sep.drop_y(real_versus_projected_y2 )\n",
    "real_versus_projected_y1 = sep.drop_y(real_versus_projected_y1 )\n",
    "#real_versus_projected_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_frames3 = [real_versus_projected_y1, real_versus_projected_y2,]\n",
    "real_versus_projected_y3 = reduce(lambda  left,right: pd.merge(left,right,on=[\"index\"],\n",
    "                                            how='outer'), data_frames3)\n",
    "real_versus_projected_y3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## optionally save of csvs of algorithms and results\n",
    "#top_based_unharmonized_on_top.to_csv('top_based_unharmonized_on_top.csv')\n",
    "#real_versus_projected_y3.to_csv('real_versus_projected_y3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optionally SAve off models (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if model folder exists and if not , then create\n",
    "model_folder = '../result_models/'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# joblib.dump(linr, ('../result_models/'+ 'unharm_top_linr.sav'))\n",
    "# joblib.dump(llreg, ('../result_models/'+ 'unharm_top_lassor.sav'))\n",
    "# joblib.dump(dtree, ('../result_models/'+ 'unharm_top_dtree.sav'))\n",
    "# joblib.dump(regr, ('../result_models/'+ 'unharm_top_regr.sav'))\n",
    "# joblib.dump(svr_p2, ('../result_models/'+ 'unharm_top_svrp2.sav'))\n",
    "# joblib.dump(etreg, ('../result_models/'+ 'unharm_top_extratree.sav'))\n",
    "# joblib.dump(eregr, ('../result_models/'+ 'unharm_top_elasticregr.sav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## Run models on other datasets : StrokeMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mri_ml_matrix = StrokeMRI.drop('participant_id', axis=1)\n",
    "X_mri = mri_ml_matrix.drop('age', axis =1)\n",
    "X_mri = X_mri.values\n",
    "X_mri = X_mri.astype('float')\n",
    "y_mri = mri_ml_matrix['age'].values\n",
    "y_mri=y_mri.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_mri_test = X_mri\n",
    "y_mri_test = y_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_mri_pred = linr.predict(X_mri_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score Linear regression: %.3f' % linr.score(X_mri_test,y_mri_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_mri_test, y_mri_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_mri_test, y_mri_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'linear regression',\n",
    "    'unharm_top_linr.sav',\n",
    "    mean_absolute_error(y_mri_test, y_mri_pred),\n",
    "    linr.score(X_mri_test,y_mri_test),\n",
    "    metrics.explained_variance_score(y_mri_test, y_mri_pred)]]\n",
    "linr_results_mri = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "linr_results_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_mri_pred = llreg.predict(X_mri_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score Lasso regression: %.3f' % llreg.score(X_mri_test,y_mri_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_mri_test, y_mri_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_mri_test, y_mri_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'lasso regression',\n",
    "    'unharm_top_llreg.sav',\n",
    "    mean_absolute_error(y_mri_test, y_mri_pred),\n",
    "    llreg.score(X_mri_test,y_mri_test),\n",
    "    metrics.explained_variance_score(y_mri_test, y_mri_pred)]]\n",
    "llreg_results_mri = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "llreg_results_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_mri_pred = dtree.predict(X_mri_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score Decision tree: %.3f' % dtree.score(X_mri_test,y_mri_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_mri_test, y_mri_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_mri_test, y_mri_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'dtree regression',\n",
    "    'unharm_top_dtree.sav',\n",
    "    mean_absolute_error(y_mri_test, y_mri_pred),\n",
    "    dtree.score(X_mri_test,y_mri_test),\n",
    "    metrics.explained_variance_score(y_mri_test, y_mri_pred)]]\n",
    "dtree_results_mri = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "dtree_results_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_mri_pred = regr.predict(X_mri_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('R2 score MLP regression: %.3f' % regr.score(X_mri_test,y_mri_test))\n",
    "print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_mri_test, y_mri_pred))\n",
    "print('The mean absolute error: %.3f' % mean_absolute_error(y_mri_test, y_mri_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'linear regression',\n",
    "    'unharm_top_linr.sav',\n",
    "    mean_absolute_error(y_mri_test, y_mri_pred),\n",
    "    regr.score(X_mri_test,y_mri_test),\n",
    "    metrics.explained_variance_score(y_mri_test, y_mri_pred)]]\n",
    "regr_results_mri = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "regr_results_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_mri_pred = svr_p2.predict(X_mri_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score SVR polynomial regression: %.3f' % svr_p2.score(X_mri_test,y_mri_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_mri_test, y_mri_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_mri_test, y_mri_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'svr pol2 regression',\n",
    "    'unharm_top_svrp2.sav',\n",
    "    mean_absolute_error(y_mri_test, y_mri_pred),\n",
    "    svr_p2.score(X_mri_test,y_mri_test),\n",
    "    metrics.explained_variance_score(y_mri_test, y_mri_pred)]]\n",
    "svr_p2_results_mri = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "svr_p2_results_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_mri_pred = eregr.predict(X_mri_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score ElasticNet CV : %.3f' % eregr.score(X_mri_test,y_mri_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_mri_test, y_mri_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_mri_test, y_mri_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'elasticnetCV',\n",
    "    'unharm_top_eregr.sav',\n",
    "    mean_absolute_error(y_mri_test, y_mri_pred),\n",
    "    eregr.score(X_mri_test,y_mri_test),\n",
    "    metrics.explained_variance_score(y_mri_test, y_mri_pred)]]\n",
    "eregr_results_mri = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#eregr_results_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_mri_pred = etreg.predict(X_mri_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score extra tree regression: %.3f' % etreg.score(X_mri_test,y_mri_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_mri_test, y_mri_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_mri_test, y_mri_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'extra trees regression',\n",
    "    'unharm_top_linr.sav',\n",
    "    mean_absolute_error(y_mri_test, y_mri_pred),\n",
    "    etreg.score(X_mri_test,y_mri_test),\n",
    "    metrics.explained_variance_score(y_mri_test, y_mri_pred)]]\n",
    "etreg_results_mri = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#etreg_results_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_based_unharmonized_on_mri =pd.concat([linr_results_mri,\n",
    "                   llreg_results_mri,\n",
    "                   dtree_results_mri,\n",
    "                   regr_results_mri,\n",
    "                   svr_p2_results_mri,\n",
    "                   eregr_results_mri,\n",
    "                   etreg_results_mri],\n",
    "                  axis=0)\n",
    "top_based_unharmonized_on_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # examoke if of plotting\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.scatter(y_test, y_pred, c='crimson')\n",
    "# p1 = max(max(y_pred), max(y_test))\n",
    "# p2 = min(min(y_pred), min(y_test))\n",
    "# plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "# plt.xlabel('True Values Age', fontsize=15)\n",
    "# plt.ylabel('Predictions', fontsize=15)\n",
    "# plt.axis('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "## Run models on other datasets: SABRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sabre_ml_matrix = SABRE.drop('participant_id', axis=1)\n",
    "X_sabre = sabre_ml_matrix.drop('age', axis =1)\n",
    "X_sabre = X_sabre.values\n",
    "X_sabre = X_sabre.astype('float')\n",
    "y_sabre = sabre_ml_matrix['age'].values\n",
    "y_sabre=y_sabre.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_sabre_test = X_sabre\n",
    "y_sabre_test = y_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_sabre_pred = linr.predict(X_sabre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score Linear regression: %.3f' % linr.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Linear Reg',\n",
    "    'unharm_top_linr.sav',\n",
    "    mean_absolute_error(y_sabre_test, y_sabre_pred),\n",
    "    linr.score(X_sabre_test,y_sabre_test),\n",
    "    metrics.explained_variance_score(y_sabre_test, y_sabre_pred)]]\n",
    "linr_results_sabre = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#linr_results_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_sabre_pred = llreg.predict(X_sabre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score Lasso regression: %.3f' % llreg.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Lasso',\n",
    "    'unharm_top_lassor.sav',\n",
    "    mean_absolute_error(y_sabre_test, y_sabre_pred),\n",
    "    llreg.score(X_sabre_test,y_sabre_test),\n",
    "    metrics.explained_variance_score(y_sabre_test, y_sabre_pred)]]\n",
    "llreg_results_sabre = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#llreg_results_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_sabre_pred = dtree.predict(X_sabre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score Decision tree: %.3f' % dtree.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Decision tree',\n",
    "    'unharm_top_svrp2.sav',\n",
    "    mean_absolute_error(y_sabre_test, y_sabre_pred),\n",
    "    dtree.score(X_sabre_test,y_sabre_test),\n",
    "    metrics.explained_variance_score(y_sabre_test, y_sabre_pred)]]\n",
    "dtree_results_sabre = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#dtree_results_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_sabre_pred = regr.predict(X_sabre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score MLP regression: %.3f' % regr.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'MLP regression',\n",
    "    'unharm_top_regr.sav',\n",
    "    mean_absolute_error(y_sabre_test, y_sabre_pred),\n",
    "    regr.score(X_sabre_test,y_sabre_test),\n",
    "    metrics.explained_variance_score(y_sabre_test, y_sabre_pred)]]\n",
    "regr_results_sabre = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#regr_results_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_sabre_pred = svr_p2.predict(X_sabre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score SVR polynomial regression: %.3f' % svr_p2.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Svr P2',\n",
    "    'unharm_top_svrp2.sav',\n",
    "    mean_absolute_error(y_sabre_test, y_sabre_pred),\n",
    "    svr_p2.score(X_sabre_test,y_sabre_test),\n",
    "    metrics.explained_variance_score(y_sabre_test, y_sabre_pred)]]\n",
    "svr_p2_results_sabre = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#svr_p2_results_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_sabre_pred = eregr.predict(X_sabre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score ElasticNet CV : %.3f' % eregr.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'ElasticnetCV',\n",
    "    'unharm_top_elasticregr.sav',\n",
    "    mean_absolute_error(y_sabre_test, y_sabre_pred),\n",
    "    eregr.score(X_sabre_test,y_sabre_test),\n",
    "    metrics.explained_variance_score(y_sabre_test, y_sabre_pred)]]\n",
    "eregr_results_sabre = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#eregr_results_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_sabre_pred = etreg.predict(X_sabre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score extra tree regression: %.3f' % etreg.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Extra trees',\n",
    "    'unharm_top_etreg.sav',\n",
    "    mean_absolute_error(y_sabre_test, y_sabre_pred),\n",
    "    etreg.score(X_sabre_test,y_sabre_test),\n",
    "    metrics.explained_variance_score(y_sabre_test, y_sabre_pred)]]\n",
    "etreg_results_sabre = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#etreg_results_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_based_unharmonized_on_sabre =pd.concat([linr_results_sabre,\n",
    "                   llreg_results_sabre,\n",
    "                   dtree_results_sabre,\n",
    "                   regr_results_sabre,\n",
    "                   svr_p2_results_sabre,\n",
    "                   eregr_results_sabre,\n",
    "                  etreg_results_sabre],\n",
    "                  axis=0)\n",
    "top_based_unharmonized_on_sabre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top_based_unharmonized_on_sabre.to_csv('top_based_unharmonized_on_sabre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "## Run models on other datasets: Insight46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insight_ml_matrix = Insight46.drop('participant_id', axis=1)\n",
    "X_insight = insight_ml_matrix.drop('age', axis =1)\n",
    "X_insight = X_insight.values\n",
    "X_insight = X_insight.astype('float')\n",
    "y_insight = insight_ml_matrix['age'].values\n",
    "y_insight=y_insight.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_insight_test = X_insight\n",
    "y_insight_test = y_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_insight_pred = linr.predict(X_insight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Linear Reg',\n",
    "    'unharm_top_linr.sav',\n",
    "    mean_absolute_error(y_insight_test, y_insight_pred),\n",
    "    linr.score(X_insight_test,y_insight_test),\n",
    "    metrics.explained_variance_score(y_insight_test, y_insight_pred)]]\n",
    "linr_results_insight = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "#linr_results_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_insight_pred = llreg.predict(X_insight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score Lasso regression: %.3f' % llreg.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Lasso',\n",
    "    'unharm_top_lassor.sav',\n",
    "    mean_absolute_error(y_insight_test, y_insight_pred),\n",
    "    llreg.score(X_insight_test,y_insight_test),\n",
    "    metrics.explained_variance_score(y_insight_test, y_insight_pred)]]\n",
    "llreg_results_insight = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "llreg_results_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_insight_pred = dtree.predict(X_insight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Decision tree',\n",
    "    'unharm_top_svrp2.sav',\n",
    "    mean_absolute_error(y_insight_test, y_insight_pred),\n",
    "    dtree.score(X_insight_test,y_insight_test),\n",
    "    metrics.explained_variance_score(y_insight_test, y_insight_pred)]]\n",
    "dtree_results_insight = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "dtree_results_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_insight_pred = regr.predict(X_insight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score MLP regression: %.3f' % regr.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'MLP regression',\n",
    "    'unharm_top_regr.sav',\n",
    "    mean_absolute_error(y_insight_test, y_insight_pred),\n",
    "    regr.score(X_insight_test,y_insight_test),\n",
    "    metrics.explained_variance_score(y_insight_test, y_insight_pred)]]\n",
    "regr_results_insight = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "regr_results_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_insight_pred = svr_p2.predict(X_insight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score SVR polynomial regression: %.3f' % svr_p2.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Svr P2',\n",
    "    'unharm_top_svrp2.sav',\n",
    "    mean_absolute_error(y_insight_test, y_insight_pred),\n",
    "    svr_p2.score(X_insight_test,y_insight_test),\n",
    "    metrics.explained_variance_score(y_insight_test, y_insight_pred)]]\n",
    "svrp2_results_insight = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "svrp2_results_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_insight_pred = eregr.predict(X_insight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('R2 score ElasticNet CV : %.3f' % eregr.score(X_sabre_test,y_sabre_test))\n",
    "# print('Explained variance score: %.3f'  % metrics.explained_variance_score(y_sabre_test, y_sabre_pred))\n",
    "# print('The mean absolute error: %.3f' % mean_absolute_error(y_sabre_test, y_sabre_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'ElasticnetCV',\n",
    "    'unharm_top_elasticregr.sav',\n",
    "    mean_absolute_error(y_insight_test, y_insight_pred),\n",
    "    eregr.score(X_insight_test,y_insight_test),\n",
    "    metrics.explained_variance_score(y_insight_test, y_insight_pred)]]\n",
    "eregr_results_insight = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "eregr_results_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_insight_pred = etreg.predict(X_insight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= [[\n",
    "    'Extra trees',\n",
    "    'unharm_top_etreg.sav',\n",
    "    mean_absolute_error(y_insight_test, y_insight_pred),\n",
    "    etreg.score(X_insight_test,y_insight_test),\n",
    "    metrics.explained_variance_score(y_insight_test, y_insight_pred)]]\n",
    "etreg_results_insight = pd.DataFrame(data, columns=['algorithm','file_name','mae', 'r2', 'explained_variance'])\n",
    "etreg_results_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_based_unharmonized_on_insight =pd.concat([linr_results_insight,\n",
    "                   llreg_results_insight,\n",
    "                   dtree_results_insight,\n",
    "                   regr_results_insight,\n",
    "                   svrp2_results_insight,\n",
    "                   eregr_results_insight,\n",
    "                  etreg_results_insight],\n",
    "                  axis=0)\n",
    "top_based_unharmonized_on_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
